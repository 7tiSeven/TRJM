version: '3.8'

# =============================================================================
# TRJM - Agentic AI Translator
# Phase 2: Local vLLM Configuration
# =============================================================================
# This configuration extends the base docker-compose.yml with vLLM service
# Usage: docker-compose -f docker-compose.yml -f docker-compose.vllm.yml up -d
# =============================================================================

services:
  # ---------------------------------------------------------------------------
  # vLLM Server with Qwen2.5-32B-Instruct
  # ---------------------------------------------------------------------------
  vllm:
    image: vllm/vllm-openai:latest
    container_name: trjm-vllm
    command: >
      --model Qwen/Qwen2.5-32B-Instruct
      --served-model-name qwen2.5-32b
      --tensor-parallel-size ${VLLM_TENSOR_PARALLEL:-1}
      --gpu-memory-utilization ${VLLM_GPU_MEMORY:-0.90}
      --max-model-len ${VLLM_MAX_MODEL_LEN:-8192}
      --host 0.0.0.0
      --port 8001
      --trust-remote-code
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HUGGING_FACE_HUB_TOKEN:-}
      - CUDA_VISIBLE_DEVICES=${CUDA_VISIBLE_DEVICES:-0}
    volumes:
      - vllm_cache:/root/.cache/huggingface
    expose:
      - "8001"
    networks:
      - trjm-network
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8001/health"]
      interval: 30s
      timeout: 30s
      retries: 5
      start_period: 120s

  # ---------------------------------------------------------------------------
  # Gateway - Override to use vLLM
  # ---------------------------------------------------------------------------
  gateway:
    environment:
      - LLM_PROVIDER=vllm
      - LLM_BASE_URL=http://vllm:8001/v1
      - LLM_API_KEY=dummy
      - LLM_MODEL=qwen2.5-32b
    depends_on:
      postgres:
        condition: service_healthy
      vllm:
        condition: service_healthy

# =============================================================================
# Additional Volumes for vLLM
# =============================================================================
volumes:
  vllm_cache:
    driver: local
